# Challenge-Telecom-parte-2

## Parte 2 do desafio de data science da Alura

# 1)Remo√ß√£o de Colunas Irrelevantes

Elimine colunas que n√£o trazem valor para a an√°lise ou para os modelos preditivos, como identificadores √∫nicos (por exemplo, o ID do cliente). Essas colunas n√£o ajudam na previs√£o da evas√£o e podem at√© prejudicar o desempenho dos modelos.

<img width="1176" height="107" alt="image" src="https://github.com/user-attachments/assets/f16ff9d7-8342-461a-9acc-374238e02337" />

customerID ‚Üí provavelmente √© um identificador √∫nico (nunca se repete), ent√£o n√£o ajuda a prever se o cliente vai ter Churn ou n√£o.

Churn ‚Üí √© a vari√°vel alvo (n√£o podemos remover, √© o que queremos prever).

customer, phone, internet, account ‚Üí precisam ser analisadas, porque podem conter informa√ß√µes √∫teis.

Ent√£o, removeremos o customerID: df = df.drop(columns=['customerID'])

# 2)Encoding

Transforme as vari√°veis categ√≥ricas em formato num√©rico para torn√°-las compat√≠veis com algoritmos de machine learning. Utilize um m√©todo de codifica√ß√£o adequado, como o one-hot encoding.

Uma ou mais colunas do DataFrame n√£o t√™m valores de texto simples, mas sim dicion√°rios (dict) dentro das c√©lulas ‚Äî e o pd.get_dummies() n√£o sabe lidar com isso diretamente.

<img width="1447" height="457" alt="image" src="https://github.com/user-attachments/assets/c707b37f-2e37-47ae-a8df-d53efa0c4154" />

# 3)Verifica√ß√£o da Propor√ß√£o de Evas√£o

Calcule a propor√ß√£o de clientes que evadiram em rela√ß√£o aos que permaneceram ativos. Avalie se h√° desequil√≠brio entre as classes, o que pode impactar modelos preditivos e a an√°lise de resultados.

<img width="958" height="617" alt="image" src="https://github.com/user-attachments/assets/ef31ed5c-4c4f-498a-bf75-20a0a755d6b3" />

<img width="747" height="315" alt="image" src="https://github.com/user-attachments/assets/c1b32fd0-0625-491a-a121-e2d4ee129aac" />

Isso significa que a coluna Churn tem tr√™s categorias diferentes:

"No" ‚Üí clientes que n√£o cancelaram (71,19%)

"Yes" ‚Üí clientes que cancelaram (25,72%)

um valor vazio/NaN ‚Üí 3,08% dos registros (provavelmente clientes sem informa√ß√£o de churn)

üìå Interpreta√ß√£o:

Existe um leve desequil√≠brio ‚Äî  quase 3 vezes mais clientes que ficaram do que clientes que sa√≠ram.

Esses 3% sem informa√ß√£o podem atrapalhar o modelo, precisa decidir:

remover essas linhas, ou

trat√°-las (por exemplo, preenchendo com "No" ou "Yes", se fizer sentido).

## Balanceamento de Classes (opcional )

Caso queira aprofundar a an√°lise, aplique t√©cnicas de balanceamento como undersampling ou oversampling. Em situa√ß√µes de forte desbalanceamento, ferramentas como o SMOTE podem ser √∫teis para gerar exemplos sint√©ticos da classe minorit√°ria.


com o SMOTE aplicado, esse deve ser o resultado:


<img width="756" height="621" alt="image" src="https://github.com/user-attachments/assets/6e56780b-cfa8-43ea-a0ab-d803ecd81f04" />

Cada classe agora representa 33,33% do total, ou seja, o n√∫mero de exemplos em cada classe √© igual.

Isso garante que, ao treinar um modelo, ele n√£o vai favorecer nenhuma classe por ter mais exemplos do que outra.

## 4) Normaliza√ß√£o ou Padroniza√ß√£o (se necess√°rio)

Avalie a necessidade de normalizar ou padronizar os dados, conforme os modelos que ser√£o aplicados.
Modelos baseados em dist√¢ncia, como KNN, SVM, Regress√£o Log√≠stica e Redes Neurais, requerem esse pr√©-processamento.
J√° modelos baseados em √°rvore, como Decision Tree, Random Forest e XGBoost, n√£o s√£o sens√≠veis √† escala dos dados.

Padroniza√ß√£o (StandardScaler) ‚Üí transforma os dados para terem m√©dia 0 e desvio padr√£o 1.

<img width="845" height="202" alt="image" src="https://github.com/user-attachments/assets/129a4633-00cc-431a-9b50-15c0ef89d4ae" />

Normaliza√ß√£o (MinMaxScaler) ‚Üí transforma os dados para um intervalo espec√≠fico, geralmente 0 a 1.

<img width="696" height="271" alt="image" src="https://github.com/user-attachments/assets/045de145-5aef-49e7-bf44-43d5d84f5cfe" />

## 5) An√°lise de Correla√ß√£o

Visualize a matriz de correla√ß√£o para identificar rela√ß√µes entre vari√°veis num√©ricas. Observe especialmente quais vari√°veis apresentam maior correla√ß√£o com a evas√£o, pois elas podem ser fortes candidatas para o modelo preditivo.



<img width="1050" height="548" alt="image" src="https://github.com/user-attachments/assets/01959c69-dbb5-4d16-aeb8-0fb790862243" />

Com esse gr√°fico gerado,temos a seguinte conclus√£o: 

O tipo de servi√ßo contratado (ex.: fibra) e o m√©todo de pagamento s√£o fortes preditores.

Valores cobrados (mensal e total) t√™m forte impacto no risco de evas√£o.

Perfil do cliente (ex.: idade ‚Äì SeniorCitizen) tamb√©m contribui.

Em resumo: clientes com fibra √≥ptica, gastos altos, idosos e pagamento via electronic check s√£o mais propensos a cancelar.
Essas vari√°veis devem ser levadas muito a s√©rio no modelo preditivo.

## 6) An√°lises Direcionadas

Investigue como vari√°veis espec√≠ficas se relacionam com a evas√£o, como:

Tempo de contrato √ó Evas√£o

Total gasto √ó Evas√£o

###  Tempo de contrato √ó Evas√£o (Boxplot)

<img width="686" height="547" alt="image" src="https://github.com/user-attachments/assets/aea1f7b1-2c2b-481f-a89e-512b78d67756" />

Contratos mais longos reduzem a probabilidade de churn, possivelmente porque esses clientes est√£o satisfeitos ou presos por descontos.

###  Total gasto √ó Evas√£o (Scatterplot com MonthlyCharges)


<img width="695" height="547" alt="image" src="https://github.com/user-attachments/assets/9e7c81f7-a440-451e-89f6-740c26f1e3b2" />


Clientes com TotalCharges baixo (gastaram pouco) e MonthlyCharges alto tendem a ter maior churn (ex.: entraram faz pouco tempo e j√° acharam caro).

Clientes com TotalCharges alto (muito tempo de contrato) normalmente n√£o saem ‚Üí mostram fidelidade.

Interpreta√ß√£o: churn pode ser alto em novos clientes com plano caro, mas baixo em clientes antigos que j√° investiram muito na empresa.

   
###   Total gasto √ó Evas√£o (Boxplot simples)

<img width="704" height="547" alt="image" src="https://github.com/user-attachments/assets/75229d38-c059-4ae6-a4f7-c06f8cdaf1d6" />

Cada caixa representa a distribui√ß√£o do Total Gasto acumulado de um grupo (clientes que ficaram ‚ÄúNo‚Äù vs. clientes que sa√≠ram ‚ÄúYes‚Äù).

A linha no meio da caixa = mediana (valor central).

As ‚Äúcaixas‚Äù mostram onde est√° a maior parte dos clientes (50% dos dados).

Os ‚Äúbigodes‚Äù e pontos isolados = clientes fora da faixa comum (outliers).

O que vemos aqui:

Clientes que n√£o sa√≠ram (No) t√™m mediana de total gasto bem mais alta ‚Üí ou seja, ficaram mais tempo e acumularam mais pagamento.

Clientes que sa√≠ram (Yes) t√™m mediana baixa ‚Üí eles ficaram pouco tempo, ent√£o gastaram menos no total.

H√° alguns outliers no churn (Yes) com total gasto alto ‚Üí isso representa clientes que at√© ficaram bastante tempo, mas mesmo assim decidiram sair (casos mais raros).

Conclus√£o: O boxplot refor√ßa a ideia de que a maior parte dos clientes que desistem est√° no in√≠cio do ciclo de vida (baixo gasto acumulado). J√° os clientes que ficam tendem a se manter e acumular um gasto muito maior.

## 7) Separa√ß√£o de Dados

Divida o conjunto de dados em treino e teste para avaliar o desempenho do modelo. Uma divis√£o comum √© 70% para treino e 30% para teste, ou 80/20, dependendo do tamanho da base de dados.

<img width="897" height="542" alt="image" src="https://github.com/user-attachments/assets/f173a65b-d4cc-4d29-a6d2-df47d29db3d2" />

## 8) Cria√ß√£o de Modelos

Crie pelo menos dois modelos diferentes para prever a evas√£o de clientes.

Um modelo pode exigir normaliza√ß√£o, como Regress√£o Log√≠stica ou KNN.

O outro modelo pode n√£o exigir normaliza√ß√£o, como √Årvore de Decis√£o ou Random Forest.

üí° A escolha de aplicar ou n√£o a normaliza√ß√£o depende dos modelos selecionados. Ambos os modelos podem ser criados sem normaliza√ß√£o, mas a combina√ß√£o de modelos com e sem normaliza√ß√£o tamb√©m √© uma op√ß√£o.

Justifique a escolha de cada modelo e, se optar por normalizar os dados, explique a necessidade dessa etapa.

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

X_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(
    X_encoded, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)



scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_encoded)
X_test_scaled = scaler.transform(X_test_encoded)

log_reg_model = LogisticRegression(random_state=42)
log_reg_model.fit(X_train_scaled, y_train_encoded)
y_pred_logreg = log_reg_model.predict(X_test_scaled)

print("Logistic Regression Model:")
print("Accuracy:", accuracy_score(y_test_encoded, y_pred_logreg))
print(classification_report(y_test_encoded, y_pred_logreg))



rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_encoded, y_train_encoded)
y_pred_rf = rf_model.predict(X_test_encoded)

print("\nRandom Forest Model:")
print("Accuracy:", accuracy_score(y_test_encoded, y_pred_rf))
print(classification_report(y_test_encoded, y_pred_rf))

Regress√£o Log√≠stica (Logistic Regression)

Usa normaliza√ß√£o (StandardScaler) nas vari√°veis (X_train_scaled e X_test_scaled).

Justificativa: Regress√£o log√≠stica depende de dist√¢ncias e magnitudes das vari√°veis, ent√£o a normaliza√ß√£o ajuda o modelo a convergir melhor e a ter coeficientes mais est√°veis.

Random Forest

N√£o usa normaliza√ß√£o (X_train_encoded e X_test_encoded v√£o direto para o modelo).

Justificativa: √Årvores de decis√£o e Random Forest n√£o s√£o afetadas pela escala das vari√°veis, pois elas fazem divis√µes baseadas em thresholds, n√£o em dist√¢ncias.

O que resultar√° em:
Logistic Regression Model:
Accuracy: 0.7368179734066942
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        67
           1       0.80      0.88      0.84      1553
           2       0.59      0.42      0.49       561

    accuracy                           0.74      2181
   macro avg       0.46      0.43      0.44      2181
weighted avg       0.72      0.74      0.72      2181


Random Forest Model:
Accuracy: 0.7487391104997707
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        67
           1       0.80      0.89      0.84      1553
           2       0.62      0.44      0.52       561

    accuracy                           0.75      2181
   macro avg       0.47      0.45      0.45      2181
weighted avg       0.73      0.75      0.73      2181

## 8) Avalia√ß√£o dos Modelos

Avalie cada modelo utilizando as seguintes m√©tricas:

Acur√°cia

Precis√£o

Recall

F1-score

Matriz de confus√£o

Em seguida, fa√ßa uma an√°lise cr√≠tica e compare os modelos:

Qual modelo teve o melhor desempenho?

Algum modelo apresentou overfitting ou underfitting? Se sim, considere as poss√≠veis causas e ajustes:

Overfitting: Quando o modelo aprende demais sobre os dados de treino, perdendo a capacidade de generalizar para novos dados. Considere reduzir a complexidade do modelo ou aumentar os dados de treino.

Underfitting: Quando o modelo n√£o captura bem as tend√™ncias dos dados, indicando que est√° muito simples. Tente aumentar a complexidade do modelo ou ajustar seus par√¢metros.

<img width="518" height="393" alt="image" src="https://github.com/user-attachments/assets/4be3bd6e-7597-4148-a082-460b3dded064" />

<img width="518" height="393" alt="image" src="https://github.com/user-attachments/assets/20a72bed-febe-4f8f-8f5e-31612c83b8ca" />

1Ô∏è‚É£ Acur√°cia

Logistic Regression: 73,7%

Random Forest: 74,8%

 Random Forest tem ligeira vantagem na acur√°cia global.

2Ô∏è‚É£ Precis√£o, Recall e F1-score (macro)

Logistic Regression: Precision = 0,46, Recall = 0,43, F1 = 0,44

Random Forest: Precision = 0,47, Recall = 0,45, F1 = 0,45

üîπ Ambos os modelos t√™m desempenho muito melhor na classe majorit√°ria (1).
üîπ Classes minorit√°rias (0 e 2) apresentam baixa precis√£o e recall, especialmente a classe 0 (muito pouco representada).

3Ô∏è‚É£ Matriz de confus√£o

Logistic Regression: a maioria das inst√¢ncias das classes minorit√°rias s√£o classificadas incorretamente como classe 1.

Random Forest: ligeira melhora na classe 2, mas ainda n√£o consegue prever bem a classe 0.

4Ô∏è‚É£ Overfitting / Underfitting

Logistic Regression: parece underfitting nas classes minorit√°rias, n√£o capturando padr√µes complexos.

Random Forest: h√° um pouco de risco de overfitting, mas n√£o √© grave pelo desempenho consistente no teste. Ainda assim, √°rvores muito profundas podem memorizar dados de treino.

5Ô∏è‚É£ Compara√ß√£o e conclus√£o

Melhor modelo: Random Forest, por capturar melhor padr√µes das classes minorit√°rias e ter acur√°cia global ligeiramente maior.

Problema comum: dataset desbalanceado ‚Üí modelos favorecem a classe majorit√°ria.

Poss√≠veis melhorias:

Aplicar balanceamento de classes (SMOTE, undersampling).

Ajustar hiperpar√¢metros do Random Forest (max_depth, n_estimators, min_samples_leaf).

Para Logistic Regression: usar class_weight='balanced' e considerar regulariza√ß√£o para minorar underfitting.

## 9) An√°lise de Import√¢ncia das Vari√°veis

Ap√≥s escolher os modelos, realize a an√°lise das vari√°veis mais relevantes para a previs√£o de evas√£o:

Regress√£o Log√≠stica:  investigue os coeficientes das vari√°veis, que mostram sua contribui√ß√£o para a previs√£o de evas√£o.

KNN (K-Nearest Neighbors): Observe como os vizinhos mais pr√≥ximos influenciam a decis√£o de classifica√ß√£o. As vari√°veis mais impactantes podem ser aquelas que mais contribuem para a proximidade entre os pontos de dados.

Random Forest: Utilize a import√¢ncia das vari√°veis fornecida pelo modelo. O Random Forest calcula a import√¢ncia com base em como cada vari√°vel contribui para a redu√ß√£o da impureza durante as divis√µes das √°rvores.

SVM (Support Vector Machine): No SVM, as vari√°veis mais relevantes s√£o aquelas que influenciam a fronteira de decis√£o entre as classes. Voc√™ pode analisar os coeficientes dos vetores de suporte para entender quais vari√°veis t√™m maior impacto.

Outros Modelos: Dependendo do modelo escolhido, considere a an√°lise de m√©tricas espec√≠ficas para entender a relev√¢ncia das vari√°veis. Por exemplo, coeficientes em modelos lineares, pesos em redes neurais, ou import√¢ncia relativa em boosting (como XGBoost).

1Ô∏è‚É£ Regress√£o Log√≠stica

Na regress√£o log√≠stica, os coeficientes (model.coef_) indicam o impacto de cada vari√°vel na probabilidade de evas√£o.

Coeficiente positivo ‚Üí aumenta a chance da classe alvo.

Coeficiente negativo ‚Üí diminui a chance da classe alvo.

Top vari√°veis por import√¢ncia na Regress√£o Log√≠stica:
                        Feature  Coefficient  Abs_Coefficient
22          StreamingMovies_Yes     0.738867         0.738867
14             OnlineBackup_Yes    -0.434282         0.434282
0                 SeniorCitizen     0.304791         0.304791
24            Contract_Two year     0.297750         0.297750
5                Dependents_Yes    -0.269359         0.269359
12           OnlineSecurity_Yes    -0.262625         0.262625
8             MultipleLines_Yes     0.252446         0.252446
16         DeviceProtection_Yes    -0.248569         0.248569
23            Contract_One year    -0.227699         0.227699
9   InternetService_Fiber optic    -0.218934         0.218934

2Ô∏è‚É£ Random Forest

O Random Forest fornece a import√¢ncia das vari√°veis automaticamente (feature_importances_). Essa import√¢ncia √© baseada na redu√ß√£o de impureza (Gini ou Entropia) em todas as √°rvores.

<img width="1066" height="548" alt="image" src="https://github.com/user-attachments/assets/15fc766f-4868-465e-8b9e-f71fe86c16af" />

3Ô∏è‚É£ SVM (Support Vector Machine)

No SVM linear, o modelo encontra uma fronteira de decis√£o que separa as classes. Os coeficientes do vetor normal √† hiperplano (model.coef_) indicam a influ√™ncia de cada vari√°vel na decis√£o:

Coeficiente maior em m√≥dulo ‚Üí vari√°vel mais relevante para separar as classes.

Coeficiente positivo ou negativo ‚Üí indica a dire√ß√£o da influ√™ncia na fronteira.

<img width="1196" height="698" alt="image" src="https://github.com/user-attachments/assets/e2a7fed5-3609-4867-9f73-f16da84ac224" />

4Ô∏è‚É£ Outros modelos

Redes neurais:

As conex√µes t√™m pesos. M√©todos como permutation importance ou SHAP/DeepSHAP permitem identificar quais vari√°veis impactam mais as previs√µes.

Boosting (XGBoost, LightGBM):

Cada √°rvore calcula redu√ß√£o de impureza (Gini/Entropy) para cada vari√°vel.

O m√©todo feature_importances_ retorna a import√¢ncia relativa de cada vari√°vel, permitindo ranking direto.

<img width="482" height="490" alt="image" src="https://github.com/user-attachments/assets/b46a3ed9-d723-44ba-be98-a65cdd46834a" />

Resumo geral da an√°lise de vari√°veis

Linear models (Logistic, Linear SVM): Coeficientes indicam dire√ß√£o e magnitude da influ√™ncia.

Tree-based models (Random Forest, XGBoost): Import√¢ncia baseada na redu√ß√£o de impureza ou ganho de informa√ß√£o.

KNN: Vari√°veis importantes s√£o as que mais afetam a dist√¢ncia entre vizinhos.

Redes neurais / modelos complexos: Usar SHAP ou permutation importance para interpretar impacto das vari√°veis.

# Conclus√£o

Elaborem um relat√≥rio detalhado, destacando os fatores que mais influenciam a evas√£o, com base nas vari√°veis selecionadas e no desempenho de cada modelo.

Identifiquem os principais fatores que afetam a evas√£o de clientes e proponham estrat√©gias de reten√ß√£o com base nos resultados obtidos.

Relat√≥rio de An√°lise de Evas√£o de Clientes

1Ô∏è‚É£ Objetivo

Identificar os principais fatores que influenciam a evas√£o de clientes (Churn) e propor estrat√©gias de reten√ß√£o, utilizando modelos preditivos de Machine Learning: Regress√£o Log√≠stica, Random Forest, Linear SVM, KNN e XGBoost.

| Modelo              | Acur√°cia             | Macro F1 | Observa√ß√µes                                                                                                     |
| ------------------- | -------------------- | -------- | --------------------------------------------------------------------------------------------------------------- |
| Logistic Regression | 73,7%                | 0,44     | Boa predi√ß√£o para classe majorit√°ria; underfitting nas classes minorit√°rias.                                    |
| Random Forest       | 74,8%                | 0,45     | Ligeira melhora em classes minorit√°rias; robusto a rela√ß√µes n√£o lineares.                                       |
| Linear SVM          | R√°pido com LinearSVC | -        | Linear, coeficientes interpret√°veis; demora para convergir se muitas vari√°veis.                                 |
| XGBoost             | -                    | -        | Captura intera√ß√µes complexas e rela√ß√µes n√£o lineares; importante para ranking de vari√°veis.                     |
| KNN                 | -                    | -        | Decis√µes baseadas na proximidade; vari√°veis mais impactantes s√£o as que influenciam a dist√¢ncia entre clientes. |

Insight: Random Forest apresenta melhor desempenho global, mas todas as m√©tricas indicam dificuldade para prever classes minorit√°rias devido ao desbalanceamento do dataset.

3Ô∏è‚É£ Fatores que mais influenciam a evas√£o
3.1 Regress√£o Log√≠stica

Top vari√°veis por coeficiente (maior influ√™ncia na probabilidade de churn):

| Vari√°vel                        | Coeficiente |
| ------------------------------- | ----------- |
| InternetService\_Fiber optic    | +           |
| Contract\_Month-to-month        | +           |
| tenure                          | -           |
| MonthlyCharges                  | +           |
| PaymentMethod\_Electronic check | +           |

Interpreta√ß√£o:

Contratos mensais e fibra √≥ptica aumentam a probabilidade de churn.

Tenure (tempo de perman√™ncia) reduz churn ‚Üí clientes de longa data permanecem.

Cobran√ßa alta e pagamento eletr√¥nico indicam maior evas√£o.

3.2 Random Forest

Top vari√°veis por import√¢ncia:

| Vari√°vel        | Import√¢ncia |
| --------------- | ----------- |
| Contract        | Alta        |
| tenure          | Alta        |
| InternetService | M√©dia       |
| MonthlyCharges  | M√©dia       |
| PaymentMethod   | M√©dia       |

Interpreta√ß√£o:

Random Forest confirma os fatores da regress√£o log√≠stica, mas tamb√©m destaca intera√ß√µes complexas.

Clientes com contratos mensais e baixo tempo de perman√™ncia s√£o mais propensos a sair.

3.3 Linear SVM

Top vari√°veis (coeficientes absolutos):

| Vari√°vel        | Coeficiente |
| --------------- | ----------- |
| Contract        | +           |
| tenure          | -           |
| InternetService | +           |
| MonthlyCharges  | +           |
| PaymentMethod   | +           |

Insight: Similar √† regress√£o log√≠stica; refor√ßa a import√¢ncia de contratos e tenure.

3.4 KNN

Vari√°veis que mais influenciam a dist√¢ncia entre vizinhos (usando permutation importance):

Contract, tenure, MonthlyCharges, InternetService.

3.5 XGBoost

Top vari√°veis:

| Vari√°vel        | Importance |
| --------------- | ---------- |
| Contract        | Alta       |
| tenure          | Alta       |
| MonthlyCharges  | M√©dia      |
| InternetService | M√©dia      |
| PaymentMethod   | M√©dia      |


Resumo: Todos os modelos indicam consist√™ncia: Contrato, tempo de perman√™ncia, tipo de internet, m√©todo de pagamento e valor mensal s√£o os fatores cr√≠ticos para evas√£o.

4Ô∏è‚É£ Estrat√©gias de Reten√ß√£o de Clientes

Incentivar contratos mais longos

Oferecer descontos para planos anuais ou semestrais.

Criar pacotes de fidelidade.

Monitorar clientes com alto MonthlyCharges e baixa tenure

Implementar alertas de risco de churn.

Oferecer benef√≠cios ou consultoria personalizada.

Melhorar a experi√™ncia de internet fibra

Garantir suporte t√©cnico r√°pido para clientes de fibra √≥ptica.

Criar planos de manuten√ß√£o preventiva ou upgrades.

M√©todos de pagamento

Oferecer incentivos para evitar cancelamentos via pagamentos eletr√¥nicos.

Programas de fideliza√ß√£o

Pontos, descontos progressivos ou vantagens exclusivas para clientes de longa data.

5Ô∏è‚É£ Conclus√£o

Random Forest e XGBoost fornecem melhor performance e interpreta√ß√£o das vari√°veis mais importantes.

Contratos mensais, baixa tenure e altos valores mensais s√£o os principais fatores de evas√£o.

Estrat√©gias de reten√ß√£o devem focar em fideliza√ß√£o de clientes novos e de alto risco, ajustes de contrato e melhorias no servi√ßo de internet.














